{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, ArrayType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"minhash\") \\\n",
    "            .getOrCreate()            \n",
    "\n",
    "sc = spark.sparkContext  # for the pre-2.0 sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<SparkContext master=local[4] appName=PySparkShell>,\n",
       " <pyspark.sql.context.SQLContext at 0x1157a8c88>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check: Spark Context for RDDS and SQL Context for Dataframes\n",
    "sc, sqlCtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash LSH example \n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.MinHashLSH  \n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/ml/min_hash_lsh_example.py   \n",
    "https://github.com/evancasey/spark-knn-recommender/blob/master/algorithms/itemSimilarity.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|\n",
      "|  3|(6,[1,4,5],[1.0,1...|\n",
      "|  4|(6,[2,3,5],[1.0,1...|\n",
      "|  5|(6,[1,2,4],[1.0,1...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "        (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "        (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),),\n",
    "        (3, Vectors.sparse(6, [1, 4, 5], [1.0, 1.0, 1.0]),),\n",
    "        (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "        (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "ex = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "print(type(ex))\n",
    "ex.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|              hashes|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|[[-6.7948028E8], ...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[-1.86843801E9],...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|[[-3.15433227E8],...|\n",
      "|  3|(6,[1,4,5],[1.0,1...|[[-6.7948028E8], ...|\n",
      "|  4|(6,[2,3,5],[1.0,1...|[[-1.86843801E9],...|\n",
      "|  5|(6,[1,2,4],[1.0,1...|[[-6.7948028E8], ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = ex.drop('hashes')\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=2, seed=123)\n",
    "m = mh.fit(ex)\n",
    "ex = m.transform(ex)\n",
    "ex.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://spark.apache.org/docs/2.1.1/ml-features.html#approximate-nearest-neighbor-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 most similar to 4: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "id=4\n",
    "key = ex.filter('id = {}'.format(id)).select('id', 'features').collect()\n",
    "# print(type(key[0][1])) # <class 'pyspark.ml.linalg.SparseVector'>\n",
    "# print(key[0][1],'\\n') # (6,[2,3,5],[1.0,1.0,1.0]) \n",
    "neighbors=2\n",
    "nn = m.approxNearestNeighbors(ex, key[0][1], neighbors+1).select('id').collect()\n",
    "print('Top {} most similar to {}:'.format(neighbors, id), [n[0] for n in nn[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = m.approxSimilarityJoin(ex, ex, .99, distCol='JaccardDistance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n",
      "|            datasetA|            datasetB|JaccardDistance|\n",
      "+--------------------+--------------------+---------------+\n",
      "|[5,(6,[1,2,4],[1....|[1,WrappedArray([...|            0.5|\n",
      "|[5,(6,[1,2,4],[1....|[0,WrappedArray([...|            0.5|\n",
      "+--------------------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+----+----+\n",
      "|            datasetA|            datasetB|JaccardDistance|id_A|id_B|\n",
      "+--------------------+--------------------+---------------+----+----+\n",
      "|[5,(6,[1,2,4],[1....|[1,WrappedArray([...|            0.5|   5|   1|\n",
      "|[5,(6,[1,2,4],[1....|[0,WrappedArray([...|            0.5|   5|   0|\n",
      "+--------------------+--------------------+---------------+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = d.withColumn('id_A', d.datasetA.id)\n",
    "d = d.withColumn('id_B', d.datasetB.id)\n",
    "d = d.filter('id_A != id_B')\n",
    "d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------+\n",
      "|id_A|combos                                       |\n",
      "+----+---------------------------------------------+\n",
      "|0   |[[2,0.5], [5,0.5], [4,0.8], [3,0.8], [1,0.8]]|\n",
      "|5   |[[1,0.5], [0,0.5], [2,0.5], [3,0.5], [4,0.8]]|\n",
      "|1   |[[4,0.5], [5,0.5], [2,0.5], [0,0.8]]         |\n",
      "|3   |[[5,0.5], [0,0.8]]                           |\n",
      "+----+---------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# d.groupby(\"id_A\").agg(F.collect_set(\"id_B\"), F.collect_list(\"id_B\",)).show()\n",
    "\n",
    "# results = d.groupby(\"id_A\").agg(F.collect_list(struct(\"id_B\", \"JaccardDistance\")).alias(\"combos\"))\n",
    "results = d.orderBy('id_A', 'JaccardDistance').groupby(\"id_A\")\\\n",
    "            .agg(F.collect_list(struct(\"id_B\", \"JaccardDistance\")).alias(\"combos\"))\n",
    "results.toPandas() # nicer output than show\n",
    "results.show(4, False) \n",
    "# False to show without truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results.withColumn('combos', explode('combos')).show() # reverses groupby/agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+---+---+---+\n",
      "| _1|                  _2| _3| _4| _5| _6|\n",
      "+---+--------------------+---+---+---+---+\n",
      "|  0|[[2,0.5], [5,0.5]...|  2|0.5|  5|0.5|\n",
      "|  5|[[1,0.5], [0,0.5]...|  1|0.5|  0|0.5|\n",
      "|  1|[[4,0.5], [5,0.5]...|  4|0.5|  5|0.5|\n",
      "|  3|  [[5,0.5], [0,0.8]]|  5|0.5|  0|0.8|\n",
      "|  2|[[5,0.5], [1,0.5]...|  5|0.5|  1|0.5|\n",
      "|  4|[[1,0.5], [0,0.8]...|  1|0.5|  0|0.8|\n",
      "+---+--------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results.withColumn('first', results.combos[0]).show(5)\n",
    "results = results.rdd.map(lambda row: row + row.combos[0] + row.combos[1]).toDF()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id|id1|id2|\n",
      "+---+---+---+\n",
      "|  0|  2|  5|\n",
      "|  5|  1|  0|\n",
      "|  1|  4|  5|\n",
      "|  3|  5|  0|\n",
      "|  2|  5|  1|\n",
      "|  4|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.withColumnRenamed('_1', 'id').withColumnRenamed('_3', 'id1').withColumnRenamed('_5', 'id2')\\\n",
    "        .select('id', 'id1', 'id2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_as1 = d.alias(\"df_as1\").select('id_A', 'id_B', 'JaccardDistance')\n",
    "# df_as2 = d.alias(\"df_as2\").select('id_A', 'id_B', 'JaccardDistance')\n",
    "# df_as1.join(df_as2, col(\"df_as1.id_A\") == col(\"df_as2.id_B\"), 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.select('id_A', 'id_B', 'JaccardDistance').coalesce(1)\\\n",
    "    .write.format(\"com.databricks.spark.csv\") .option(\"header\", \"true\").save(\"example.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash LSH with NPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cdf = spark.read.csv('s3n://gschoolcapstone/npidata_20050523-20170813_clean.csv', \\\n",
    "#                      header=True, inferSchema=True)\n",
    "cdf = spark.read.csv('npidata_20050523-20170813_clean.csv', \\\n",
    "                     header=True, inferSchema=True).limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = cdf.rdd\n",
    "npi = rdd.map(lambda x: x[0])\n",
    "features = rdd.map(lambda x: x[1:])\n",
    "feature_cols = cdf.columns[1:]\n",
    "# feature_cols = list(feature_cols.asDict().values())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "cdf = va.transform(cdf)\n",
    "# cdf.select(\"NPI\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cdf = cdf.drop('hashes')\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10, seed=123)\n",
    "model = mh.fit(cdf)\n",
    "cdf = model.transform(cdf)\n",
    "# cdf.select(\"NPI\", \"features\", \"hashes\").show(5)\n",
    "# cdf.select('NPI','features', 'hashes').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "# stages = []\n",
    "# stages.append(VectorAssembler(inputCols=feature_cols, outputCol='features'))\n",
    "# hash_slices = 4\n",
    "# stages.append(MinHashLSH(inputCol='features', outputCol='hashes', numHashTables=hash_slices, seed=123))\n",
    "# pipeline = Pipeline(stages=stages)\n",
    "# model = pipeline.fit(cdf)\n",
    "# cdf = model.transform(cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances = model.approxSimilarityJoin(cdf, cdf, .5, distCol='JaccardDistance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JaccardDistance', 'NPI', 'NPI_similar']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = distances.withColumn('NPI', distances.datasetA.NPI)\n",
    "distances = distances.withColumn('NPI_similar', distances.datasetB.NPI)\n",
    "distances = distances.drop('datasetA', 'datasetB')\n",
    "distances.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distances.select('NPI', 'NPI_similar', 'JaccardDistance').coalesce(1)\\\n",
    "#     .write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3n://gschoolcapstone/distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances.select('NPI', 'NPI_similar', 'JaccardDistance').write.csv('distances', \\\n",
    "                                                                    header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viewing results in notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distances.orderBy('NPI', 'JaccardDistance').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distances.orderBy('NPI', 'JaccardDistance').filter('NPI != NPI_similar').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances.createOrReplaceTempView(\"sim\")\n",
    "# spark.sql(\"SELECT NPI, NPI_similar, JaccardDistance FROM sim \\\n",
    "#             WHERE NPI != NPI_similar \\\n",
    "#             ORDER BY NPI, JaccardDistance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------------+\n",
      "|       NPI|NPI_similar|   JaccardDistance|\n",
      "+----------+-----------+------------------+\n",
      "|1215930367| 1215930268|               0.0|\n",
      "|1215930367| 1851394886|0.1428571428571429|\n",
      "|1215930367| 1629071667|0.1428571428571429|\n",
      "|1215930367| 1336142553|              0.25|\n",
      "|1215930367| 1558364588|              0.25|\n",
      "|1215930367| 1326041500|0.2857142857142857|\n",
      "|1215930367| 1013910272|             0.375|\n",
      "|1215930367| 1316940588|             0.375|\n",
      "|1215930367| 1992708168|             0.375|\n",
      "|1215930367| 1003819269|             0.375|\n",
      "|1215930367| 1699778860|             0.375|\n",
      "|1215930367| 1174526347|             0.375|\n",
      "|1215930367| 1740283738|             0.375|\n",
      "|1215930367| 1740283753|             0.375|\n",
      "|1215930367| 1881697894|             0.375|\n",
      "|1215930367| 1871596098|             0.375|\n",
      "|1215930367| 1710980941|             0.375|\n",
      "|1215930367| 1124021134|             0.375|\n",
      "|1215930367| 1114920238|             0.375|\n",
      "|1215930367| 1295738219|             0.375|\n",
      "+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT NPI, NPI_similar, JaccardDistance FROM sim \\\n",
    "            WHERE NPI != NPI_similar AND NPI == 1215930367 \\\n",
    "            ORDER BY NPI, JaccardDistance \\\n",
    "            LIMIT 20\").show()\n",
    "\n",
    "# 1215930367 - 1306849450 (.375), 1588667638 (.555), 1679576722 (.666)\n",
    "# 1578547329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npi=1215930367\n",
    "key = cdf.filter('NPI = {}'.format(npi)).select('NPI', 'features').collect()\n",
    "# print(type(key[0][1])) # <class 'pyspark.ml.linalg.SparseVector'>\n",
    "# print(key[0][1],'\\n') # (6,[2,3,5],[1.0,1.0,1.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most similar to 1215930367: [1952305591, 1215930268, 1932140159, 1235168808, 1962406462, 1215930367, 1669612123, 1639278559, 1972608594, 1639273956, 1356383103, 1114020922, 1164529806, 1609972801, 1700980299, 1821195132, 1184723827, 1376641308, 1750480950, 1194829267]\n"
     ]
    }
   ],
   "source": [
    "neighbors=20\n",
    "nn = model.approxNearestNeighbors(cdf, key[0][1], neighbors+1, distCol='NeighborDistance').select('NPI').collect()\n",
    "print('Top {} most similar to {}:'.format(neighbors, npi), [n[0] for n in nn[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
