{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, time\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, ArrayType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .appName(\"capstone\") \\\n",
    "            .getOrCreate()\n",
    "#             .master(\"local[64]\") \\\n",
    "#             .config('spark.driver.extraClassPath','postgresql-9.1-901-1.jdbc4.jar') \\\n",
    "            \n",
    "\n",
    "sc = spark.sparkContext  # for the pre-2.0 sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<SparkContext master=local[*] appName=PySparkShell>,\n",
       " <pyspark.sql.context.SQLContext at 0x7fb02a423978>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if Spark Context is running --> RDDS and SQL Context is running --> Dataframes\n",
    "sc, sqlCtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# schema = StructType( [\n",
    "#     StructField('NPI', IntegerType(), True), \n",
    "#     StructField('Entity Type Code', IntegerType(), True),\n",
    "#     StructField('Replacement NPI', IntegerType(), True)\n",
    "#     ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link = 's3n://gschoolcapstone/npidata_20050523-20170813.csv'\n",
    "# .option(\"maxColumns\", 309)\n",
    "df = spark.read.csv(link, header=True, inferSchema=True)\n",
    "# .limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.write.json('subset')\n",
    "# df.write.format('json').save('../data/subset.json')\n",
    "# df = spark.read.json('../data/subset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns in proper format\n",
    "cols = df.columns\n",
    "new_cols = [col.replace('(', '').replace(')', '').replace('.', '').replace(' ', '_') for col in cols]\n",
    "for old, new in zip(cols, new_cols):\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"npi\")\n",
    "# df.registerTempTable('npi')\n",
    "# spark.sql('SELECT * FROM npi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter where NPIs are active... do updates have reactivated providers?\n",
    "df = df.filter(df.Entity_Type_Code.isNotNull())\n",
    "# spark.sql('SELECT Entity_Type_Code FROM npi GROUP BY Entity_Type_Code').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Provider_Gender_Code: M, F, null, GUTHMILLER\n",
    "# df.select('Provider_Gender_Code').filter(\"Provider_Gender_Code == 'GUTHMILLER'\").show()\n",
    "df = df.withColumn('Gender', regexp_replace(df.Provider_Gender_Code, 'GUTHMILLER', 'X'))\n",
    "# df = df.regexp_replace('GUTHMILLER', 'X', subset='Provider_Gender_Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.createOrReplaceTempView(\"npi\")\n",
    "# spark.sql('SELECT Gender FROM npi GROUP BY Gender').show()\n",
    "# spark.sql('SELECT Provider_Gender_Code FROM npi GROUP BY Provider_Gender_Code').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill na values\n",
    "na_dict = {'Gender': 'X', \n",
    "           'Is_Sole_Proprietor': 'X', \n",
    "           'Is_Organization_Subpart': 'X',\n",
    "           'Provider_Credential_Text': 'X'}\n",
    "#            'Healthcare_Provider_Taxonomy_Code_1': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_2': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_3': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_4': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_5': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_6': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_7': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_8': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_9': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_10': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_11': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_12': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_13': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_14': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_15': 'NA', \n",
    "df = df.na.fill(na_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatting(x):\n",
    "    x = x.upper()\n",
    "    x = re.sub(re.compile('\\.|\\>|\\`'), '', x)\n",
    "    x = re.sub(re.compile('\\,|\\;|\\-|\\(|\\)|\\/'), ' ', x)\n",
    "    x = re.sub(re.compile('\\s+'), ' ', x)\n",
    "#     x = x.replace('M D', 'MD')\n",
    "    x = re.sub(\"M D\", \"MD\", x)\n",
    "    x = re.sub(\"D C\", \"DC\", x)\n",
    "    x = re.sub(\"P C\", \"PC\", x)\n",
    "    x = re.sub(\"D P M\", \"DPM\", x)\n",
    "    x = re.sub(\"D O\", \"DO\", x)\n",
    "    x = re.sub(\"O D\", \"OD\", x)\n",
    "    x = re.sub(\"0D\", \"OD\", x)\n",
    "    x = re.sub(\"PHARMD\", \"RPH \", x)\n",
    "    x = re.sub(\"PHYSICIAN ASSISTANT\", \"PA\", x)\n",
    "    x = re.sub(\"NURSE PRACTITIONER\", \"NP\", x)\n",
    "    x = re.sub(\"PHYSICAL THERAPIST\", \"PT\", x)\n",
    "    x = re.sub(\"(BS IN PHARMACY|BS PHARMACY|DOCTOR OF PHARMACY|PHARMACIST|PHARMD)\", \" RPH \", x)\n",
    "    x = re.sub(\"[\\d]\", \"\", x) # remove numbers\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "format_udf = udf(formatting, StringType())\n",
    "# print(formatting('hey. this is john. . . .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.withColumn('test', format_udf(col('Provider_Credential_Text')))\n",
    "# df.show(5)\n",
    "# test = df.withColumn('new', regexp_replace(df.Provider_Credential_Text, '\\.', ''))\n",
    "# test.select('Provider_Credential_Text','new').show(10)\n",
    "\n",
    "df = df.withColumn('Credentials', format_udf(df.Provider_Credential_Text))\n",
    "# df.select('Provider_Credential_Text', 'Credentials').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select('Gender','Is_Sole_Proprietor','Is_Organization_Subpart','Credentials').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark pipeline to get feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stages, feature_cols = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in my_cols:\n",
    "    stages.append(StringIndexer(inputCol=col, outputCol=col+'_idx', handleInvalid='error'))\n",
    "    stages.append(OneHotEncoder(dropLast=True, inputCol=col+'_idx', outputCol=col+'_ohe'))\n",
    "    feature_cols.append(col+'_ohe')\n",
    "stages.append(VectorAssembler(inputCols=feature_cols, outputCol='features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.drop('Provider_Gender_Code_idx')\n",
    "# df = df.drop('Provider_Gender_Code_ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = 'Provider_Gender_Code'\n",
    "stridx = StringIndexer(inputCol=col, outputCol=col+'_idx', handleInvalid='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = stridx.fit(df)\n",
    "# df = model.transform(df)\n",
    "# {i: label for i, label in enumerate(model.labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.select('Gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(dropLast=True, inputCol=col+'_idx', outputCol=col+'_ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = ohe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.select('Gender').show()\n",
    "# df.select('Gender_').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [col+'_ohe']\n",
    "va = VectorAssembler(inputCols=features, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://spark.apache.org/docs/1.6.1/ml-guide.html#example-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [stridx, ohe, va])\n",
    "model = pipeline.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select('NPI', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache processed dataframe/model\n",
    "df.persist() \n",
    "# df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "# df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash LSH example\n",
    "https://janzhou.org/lsh/   \n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.MinHashLSH  \n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/ml/min_hash_lsh_example.py   \n",
    "https://github.com/evancasey/spark-knn-recommender/blob/master/algorithms/itemSimilarity.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|\n",
      "|  3|(6,[1,4,5],[1.0,1...|\n",
      "|  4|(6,[2,3,5],[1.0,1...|\n",
      "|  5|(6,[1,2,4],[1.0,1...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "        (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "        (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),),\n",
    "        (3, Vectors.sparse(6, [1, 4, 5], [1.0, 1.0, 1.0]),),\n",
    "        (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "        (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "ex = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "print(type(ex))\n",
    "ex.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|              hashes|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|[[-6.7948028E8], ...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[-1.86843801E9],...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|[[-3.15433227E8],...|\n",
      "|  3|(6,[1,4,5],[1.0,1...|[[-6.7948028E8], ...|\n",
      "|  4|(6,[2,3,5],[1.0,1...|[[-1.86843801E9],...|\n",
      "|  5|(6,[1,2,4],[1.0,1...|[[-6.7948028E8], ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = ex.drop('hashes')\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=2, seed=123)\n",
    "m = mh.fit(ex)\n",
    "ex = m.transform(ex)\n",
    "ex.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://spark.apache.org/docs/2.1.1/ml-features.html#approximate-nearest-neighbor-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 most similar to 4: [1, 0]\n"
     ]
    }
   ],
   "source": [
    "id=4\n",
    "key = ex.filter('id = {}'.format(id)).select('id', 'features').collect()\n",
    "# print(type(key[0][1])) # <class 'pyspark.ml.linalg.SparseVector'>\n",
    "# print(key[0][1],'\\n') # (6,[2,3,5],[1.0,1.0,1.0]) \n",
    "neighbors=2\n",
    "nn = m.approxNearestNeighbors(ex, key[0][1], neighbors+1).select('id').collect()\n",
    "print('Top {} most similar to {}:'.format(neighbors, id), [n[0] for n in nn[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = m.approxSimilarityJoin(ex, ex, .99, distCol='JaccardDistance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n",
      "|            datasetA|            datasetB|JaccardDistance|\n",
      "+--------------------+--------------------+---------------+\n",
      "|[5,(6,[1,2,4],[1....|[1,WrappedArray([...|            0.5|\n",
      "|[5,(6,[1,2,4],[1....|[0,WrappedArray([...|            0.5|\n",
      "+--------------------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+----+----+\n",
      "|            datasetA|            datasetB|JaccardDistance|id_A|id_B|\n",
      "+--------------------+--------------------+---------------+----+----+\n",
      "|[5,(6,[1,2,4],[1....|[1,WrappedArray([...|            0.5|   5|   1|\n",
      "|[5,(6,[1,2,4],[1....|[0,WrappedArray([...|            0.5|   5|   0|\n",
      "+--------------------+--------------------+---------------+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = d.withColumn('id_A', d.datasetA.id)\n",
    "d = d.withColumn('id_B', d.datasetB.id)\n",
    "d = d.filter('id_A != id_B')\n",
    "d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------+\n",
      "|id_A|combos                                       |\n",
      "+----+---------------------------------------------+\n",
      "|0   |[[2,0.5], [5,0.5], [4,0.8], [3,0.8], [1,0.8]]|\n",
      "|5   |[[1,0.5], [0,0.5], [2,0.5], [3,0.5], [4,0.8]]|\n",
      "|1   |[[4,0.5], [5,0.5], [2,0.5], [0,0.8]]         |\n",
      "|3   |[[5,0.5], [0,0.8]]                           |\n",
      "+----+---------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# d.groupby(\"id_A\").agg(F.collect_set(\"id_B\"), F.collect_list(\"id_B\",)).show()\n",
    "\n",
    "# results = d.groupby(\"id_A\").agg(F.collect_list(struct(\"id_B\", \"JaccardDistance\")).alias(\"combos\"))\n",
    "results = d.orderBy('id_A', 'JaccardDistance').groupby(\"id_A\")\\\n",
    "            .agg(F.collect_list(struct(\"id_B\", \"JaccardDistance\")).alias(\"combos\"))\n",
    "results.toPandas() # nicer output than show\n",
    "results.show(4, False) \n",
    "# False to show without truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results.withColumn('combos', explode('combos')).show() # reverses groupby/agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+---+---+---+\n",
      "| _1|                  _2| _3| _4| _5| _6|\n",
      "+---+--------------------+---+---+---+---+\n",
      "|  0|[[2,0.5], [5,0.5]...|  2|0.5|  5|0.5|\n",
      "|  5|[[1,0.5], [0,0.5]...|  1|0.5|  0|0.5|\n",
      "|  1|[[4,0.5], [5,0.5]...|  4|0.5|  5|0.5|\n",
      "|  3|  [[5,0.5], [0,0.8]]|  5|0.5|  0|0.8|\n",
      "|  2|[[5,0.5], [1,0.5]...|  5|0.5|  1|0.5|\n",
      "|  4|[[1,0.5], [0,0.8]...|  1|0.5|  0|0.8|\n",
      "+---+--------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results.withColumn('first', results.combos[0]).show(5)\n",
    "results = results.rdd.map(lambda row: row + row.combos[0] + row.combos[1]).toDF()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id|id1|id2|\n",
      "+---+---+---+\n",
      "|  0|  2|  5|\n",
      "|  5|  1|  0|\n",
      "|  1|  4|  5|\n",
      "|  3|  5|  0|\n",
      "|  2|  5|  1|\n",
      "|  4|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.withColumnRenamed('_1', 'id').withColumnRenamed('_3', 'id1').withColumnRenamed('_5', 'id2')\\\n",
    "        .select('id', 'id1', 'id2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_as1 = d.alias(\"df_as1\").select('id_A', 'id_B', 'JaccardDistance')\n",
    "# df_as2 = d.alias(\"df_as2\").select('id_A', 'id_B', 'JaccardDistance')\n",
    "# df_as1.join(df_as2, col(\"df_as1.id_A\") == col(\"df_as2.id_B\"), 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.select('id_A', 'id_B', 'JaccardDistance').coalesce(1)\\\n",
    "    .write.format(\"com.databricks.spark.csv\") .option(\"header\", \"true\").save(\"example.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with NPI data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cdf = spark.read.csv('s3n://gschoolcapstone/npidata_20050523-20170813_clean.csv', \\\n",
    "#                      header=True, inferSchema=True)\n",
    "cdf = spark.read.csv('npidata_20050523-20170813_clean.csv', \\\n",
    "                     header=True, inferSchema=True).limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = cdf.rdd\n",
    "npi = rdd.map(lambda x: x[0])\n",
    "features = rdd.map(lambda x: x[1:])\n",
    "feature_cols = cdf.columns[1:]\n",
    "# feature_cols = list(feature_cols.asDict().values())[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "cdf = va.transform(cdf)\n",
    "# cdf.select(\"NPI\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cdf = cdf.drop('hashes')\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10, seed=123)\n",
    "model = mh.fit(cdf)\n",
    "cdf = model.transform(cdf)\n",
    "# cdf.select(\"NPI\", \"features\", \"hashes\").show(5)\n",
    "# cdf.select('NPI','features', 'hashes').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with pipeline... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stages = []\n",
    "# stages.append(VectorAssembler(inputCols=feature_cols, outputCol='features'))\n",
    "# hash_slices = 4\n",
    "# stages.append(MinHashLSH(inputCol='features', outputCol='hashes', numHashTables=hash_slices, seed=123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cdf = cdf.drop('features', 'hashes')\n",
    "# pipeline = Pipeline(stages=stages)\n",
    "# model = pipeline.fit(cdf)\n",
    "# cdf = model.transform(cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding similarity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances = model.approxSimilarityJoin(cdf, cdf, .5, distCol='JaccardDistance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JaccardDistance', 'NPI', 'NPI_similar']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = distances.withColumn('NPI', distances.datasetA.NPI)\n",
    "distances = distances.withColumn('NPI_similar', distances.datasetB.NPI)\n",
    "distances = distances.drop('datasetA', 'datasetB')\n",
    "distances.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write results to csv..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distances.select('NPI', 'NPI_similar', 'JaccardDistance').coalesce(1)\\\n",
    "#     .write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3n://gschoolcapstone/distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances.select('NPI', 'NPI_similar', 'JaccardDistance').write.csv('distances', \\\n",
    "                                                                    header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "write results to Postgres database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_writer = DataFrameWriter(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format jdbc:postgresql://host:port/database\n",
    "url = \"jdbc:postgresql://128.177.113.102:5432/capstone\"\n",
    "table = \"distances10000\"\n",
    "mode = \"overwrite\" # or \"append\"\n",
    "properties = {\"user\":\"postgres\", \"password\":\"postgres\", \"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_writer.jdbc(url, table, mode, properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OR\n",
    "# distances.write.jdbc(url=url, table=\"similarity\", mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viewing results in notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distances.orderBy('NPI', 'JaccardDistance').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distances.orderBy('NPI', 'JaccardDistance').filter('NPI != NPI_similar').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances.createOrReplaceTempView(\"sim\")\n",
    "# spark.sql(\"SELECT NPI, NPI_similar, JaccardDistance FROM sim \\\n",
    "#             WHERE NPI != NPI_similar \\\n",
    "#             ORDER BY NPI, JaccardDistance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------------+\n",
      "|       NPI|NPI_similar|   JaccardDistance|\n",
      "+----------+-----------+------------------+\n",
      "|1215930367| 1215930268|               0.0|\n",
      "|1215930367| 1851394886|0.1428571428571429|\n",
      "|1215930367| 1629071667|0.1428571428571429|\n",
      "|1215930367| 1336142553|              0.25|\n",
      "|1215930367| 1558364588|              0.25|\n",
      "|1215930367| 1326041500|0.2857142857142857|\n",
      "|1215930367| 1013910272|             0.375|\n",
      "|1215930367| 1316940588|             0.375|\n",
      "|1215930367| 1992708168|             0.375|\n",
      "|1215930367| 1003819269|             0.375|\n",
      "|1215930367| 1699778860|             0.375|\n",
      "|1215930367| 1174526347|             0.375|\n",
      "|1215930367| 1740283738|             0.375|\n",
      "|1215930367| 1740283753|             0.375|\n",
      "|1215930367| 1881697894|             0.375|\n",
      "|1215930367| 1871596098|             0.375|\n",
      "|1215930367| 1710980941|             0.375|\n",
      "|1215930367| 1124021134|             0.375|\n",
      "|1215930367| 1114920238|             0.375|\n",
      "|1215930367| 1295738219|             0.375|\n",
      "+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT NPI, NPI_similar, JaccardDistance FROM sim \\\n",
    "            WHERE NPI != NPI_similar AND NPI == 1215930367 \\\n",
    "            ORDER BY NPI, JaccardDistance \\\n",
    "            LIMIT 20\").show()\n",
    "\n",
    "# 1215930367 - 1306849450 (.375), 1588667638 (.555), 1679576722 (.666)\n",
    "# 1578547329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npi=1215930367\n",
    "key = cdf.filter('NPI = {}'.format(npi)).select('NPI', 'features').collect()\n",
    "# print(type(key[0][1])) # <class 'pyspark.ml.linalg.SparseVector'>\n",
    "# print(key[0][1],'\\n') # (6,[2,3,5],[1.0,1.0,1.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most similar to 1215930367: [1952305591, 1215930268, 1932140159, 1235168808, 1962406462, 1215930367, 1669612123, 1639278559, 1972608594, 1639273956, 1356383103, 1114020922, 1164529806, 1609972801, 1700980299, 1821195132, 1184723827, 1376641308, 1750480950, 1194829267]\n"
     ]
    }
   ],
   "source": [
    "neighbors=20\n",
    "nn = model.approxNearestNeighbors(cdf, key[0][1], neighbors+1, distCol='NeighborDistance').select('NPI').collect()\n",
    "print('Top {} most similar to {}:'.format(neighbors, npi), [n[0] for n in nn[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib Similarity Matrix (depracated)\n",
    "https://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_rdd = sc.parallelize([[1, 2], [1, 5]])\n",
    "# RowMatrix(rdd, numRows=0, numCols=0)\n",
    "mat = RowMatrix(row_rdd)\n",
    "sims = mat.columnSimilarities()\n",
    "sims.entries.first().value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(features.take(1)[0]) # 938\n",
    "features.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = RowMatrix(features, numRows=5205376, numCols=938)\n",
    "sims = mat.columnSimilarities()\n",
    "sims.entries.first().value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
