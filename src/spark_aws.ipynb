{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking if Spark Context is running --> RDDS and SQL Context is running --> Dataframes\n",
    "# sc, sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[8]\") \\\n",
    "            .appName(\"capstone\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # for the pre-2.0 sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# schema = StructType( [\n",
    "#     StructField('NPI', IntegerType(), True), \n",
    "#     StructField('Entity Type Code', IntegerType(), True),\n",
    "#     StructField('Replacement NPI', IntegerType(), True)\n",
    "#     ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link = 's3n://gschoolcapstone/npidata_20050523-20170813.csv'\n",
    "# .option(\"maxColumns\", 309)\n",
    "df = spark.read.csv(link, header=True, inferSchema=True)\n",
    "# .limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.dtypes\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.write.json('subset')\n",
    "# df.write.format('json').save('../data/subset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('../data/subset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pdf = pd.read_csv(link, nrows=1000)\n",
    "# pdf['Provider Business Mailing Address State Name'].unique()\n",
    "# len(pdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns in proper format\n",
    "cols = df.columns\n",
    "new_cols = [col.replace('(', '').replace(')', '').replace('.', '').replace(' ', '_') for col in cols]\n",
    "for old, new in zip(cols, new_cols):\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"npi\")\n",
    "# df.registerTempTable('npi')\n",
    "# spark.sql('SELECT * FROM npi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter where NPIs are active... do updates have reactivated providers?\n",
    "df = df.filter(df.Entity_Type_Code.isNotNull())\n",
    "# spark.sql('SELECT Entity_Type_Code FROM npi GROUP BY Entity_Type_Code').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Provider_Gender_Code|\n",
      "+--------------------+\n",
      "|                   F|\n",
      "|                null|\n",
      "|                   M|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Provider_Gender_Code: M, F, null, GUTHMILLER\n",
    "# df.select('Provider_Gender_Code').filter(\"Provider_Gender_Code == 'GUTHMILLER'\").show()\n",
    "df = df.replace('GUTHMILLER', 'X', subset='Provider_Gender_Code')\n",
    "spark.sql('SELECT Provider_Gender_Code FROM npi GROUP BY Provider_Gender_Code').show()\n",
    "\n",
    "# HELP: NOT REPLACING STRING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|Provider_Credential_Text|\n",
      "+------------------------+\n",
      "|                    M.D.|\n",
      "|                      MD|\n",
      "|                       X|\n",
      "|                    M.D.|\n",
      "|                    M.D.|\n",
      "|                       X|\n",
      "|                      MD|\n",
      "|                  MA-CCC|\n",
      "|                      MD|\n",
      "|                   M. D.|\n",
      "+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill na values\n",
    "na_dict = {'Provider_Gender_Code': 'X', \n",
    "           'Is_Sole_Proprietor': 'X', \n",
    "           'Is_Organization_Subpart': 'X',\n",
    "           'Provider_Credential_Text': 'X'}\n",
    "#            'Healthcare_Provider_Taxonomy_Code_1': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_2': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_3': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_4': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_5': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_6': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_7': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_8': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_9': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_10': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_11': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_12': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_13': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_14': 'NA',\n",
    "#            'Healthcare_Provider_Taxonomy_Code_15': 'NA', \n",
    "df = df.na.fill(na_dict)\n",
    "df.select('Provider_Credential_Text').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatting(x):\n",
    "    x = re.sub(re.compile('\\.'), '', x)\n",
    "    x = re.sub(re.compile('\\s'), '', x)\n",
    "    x = x.replace('M D', 'MD')\n",
    "    return x\n",
    "\n",
    "format_udf = udf(formatting, StringType())\n",
    "# print(formatting('hey. this is john. . . .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|Provider_Credential_Text|   new|\n",
      "+------------------------+------+\n",
      "|                    M.D.|    MD|\n",
      "|                      MD|    MD|\n",
      "|                       X|     X|\n",
      "|                    M.D.|    MD|\n",
      "|                    M.D.|    MD|\n",
      "|                       X|     X|\n",
      "|                      MD|    MD|\n",
      "|                  MA-CCC|MA-CCC|\n",
      "|                      MD|    MD|\n",
      "|                   M. D.|    MD|\n",
      "+------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = df.withColumn('test', format_udf(col('Provider_Credential_Text')))\n",
    "# df.show(5)\n",
    "# test = df.withColumn('new', regexp_replace(df.Provider_Credential_Text, '\\.', ''))\n",
    "# test.select('Provider_Credential_Text','new').show(10)\n",
    "\n",
    "test = df.withColumn('new', format_udf(df.Provider_Credential_Text))\n",
    "test.select('Provider_Credential_Text', 'new').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark pipeline to get feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stages, feature_cols = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in my_cols:\n",
    "    stages.append(StringIndexer(inputCol=col, outputCol=col+'_idx', handleInvalid='error'))\n",
    "    stages.append(OneHotEncoder(dropLast=True, inputCol=col+'_idx', outputCol=col+'_ohe'))\n",
    "    feature_cols.append(col+'_ohe')\n",
    "stages.append(VectorAssembler(inputCols=feature_cols, outputCol='features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.drop('Provider_Gender_Code_idx')\n",
    "# df = df.drop('Provider_Gender_Code_ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = 'Provider_Gender_Code'\n",
    "stridx = StringIndexer(inputCol=col, outputCol=col+'_idx', handleInvalid='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = stridx.fit(df)\n",
    "# df = model.transform(df)\n",
    "# {i: label for i, label in enumerate(model.labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.select('Gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(dropLast=True, inputCol=col+'_idx', outputCol=col+'_ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = ohe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.select('Gender').show()\n",
    "# df.select('Gender_').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [col+'_ohe']\n",
    "va = VectorAssembler(inputCols=features, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://spark.apache.org/docs/1.6.1/ml-guide.html#example-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [stridx, ohe, va])\n",
    "model = pipeline.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select('NPI', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache processed dataframe/model\n",
    "# df.persist() \n",
    "# df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "# df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash LSH example\n",
    "https://janzhou.org/lsh/   \n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.MinHashLSH  \n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/ml/min_hash_lsh_example.py   \n",
    "https://github.com/evancasey/spark-knn-recommender/blob/master/algorithms/itemSimilarity.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, SparseVector(6, {0: 1.0, 1: 1.0, 2: 1.0})),\n",
       " (1, SparseVector(6, {2: 1.0, 3: 1.0, 4: 1.0})),\n",
       " (2, SparseVector(6, {0: 1.0, 2: 1.0, 4: 1.0})),\n",
       " (3, SparseVector(6, {1: 1.0, 3: 1.0, 5: 1.0})),\n",
       " (4, SparseVector(6, {2: 1.0, 3: 1.0, 5: 1.0})),\n",
       " (5, SparseVector(6, {1: 1.0, 2: 1.0, 4: 1.0}))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "        (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "        (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),),\n",
    "        (3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "        (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "        (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|\n",
      "|  3|(6,[1,3,5],[1.0,1...|\n",
      "|  4|(6,[2,3,5],[1.0,1...|\n",
      "|  5|(6,[1,2,4],[1.0,1...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "print(type(ex))\n",
    "ex.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|              hashes|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|[[-6.7948028E8], ...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[-1.86843801E9],...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|[[-3.15433227E8],...|\n",
      "|  3|(6,[1,3,5],[1.0,1...|[[-1.86843801E9],...|\n",
      "|  4|(6,[2,3,5],[1.0,1...|[[-1.86843801E9],...|\n",
      "|  5|(6,[1,2,4],[1.0,1...|[[-6.7948028E8], ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = ex.drop('hashes')\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=4, seed=123)\n",
    "model = mh.fit(ex)\n",
    "ex = model.transform(ex)\n",
    "ex.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://spark.apache.org/docs/2.1.1/ml-features.html#approximate-nearest-neighbor-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id=4\n",
    "key = ex.filter('id = {}'.format(id)).select('id', 'features').collect()\n",
    "# print(type(key[0][1])) # <class 'pyspark.ml.linalg.SparseVector'>\n",
    "# print(key[0][1],'\\n') # (6,[2,3,5],[1.0,1.0,1.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 most similar to 4: [3, 1]\n"
     ]
    }
   ],
   "source": [
    "neighbors=2\n",
    "nn = model.approxNearestNeighbors(ex, key[0][1], neighbors+1, distCol='JaccardDistance').select('id').collect()\n",
    "print('Top {} most similar to {}:'.format(neighbors, id), [n[0] for n in nn[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with NPI data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = spark.read.csv('../data/npidata_20050523-20170813_clean_dont_overwrite.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols = cdf.columns[1:]\n",
    "# feature_cols = list(feature_cols.asDict().values())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[537] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = cdf.rdd\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[538] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npi = rdd.map(lambda x: x[0])\n",
    "features = rdd.map(lambda x: x[1:])\n",
    "features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|       NPI|            features|\n",
      "+----------+--------------------+\n",
      "|1679576722|(938,[0,2,37,362,...|\n",
      "|1588667638|(938,[0,2,5,17,31...|\n",
      "|1497758544|(938,[1,7,35,662]...|\n",
      "|1306849450|(938,[0,2,5,51,46...|\n",
      "|1215930367|(938,[0,2,5,51,23...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "cdf = va.transform(cdf)\n",
    "cdf.select(\"NPI\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|       NPI|            features|              hashes|\n",
      "+----------+--------------------+--------------------+\n",
      "|1679576722|(938,[0,2,37,362,...|[[-1.775142435E9]...|\n",
      "|1588667638|(938,[0,2,5,17,31...|[[-1.775142435E9]...|\n",
      "|1497758544|(938,[1,7,35,662]...|[[-6.7948028E8], ...|\n",
      "|1306849450|(938,[0,2,5,51,46...|[[-1.775142435E9]...|\n",
      "|1215930367|(938,[0,2,5,51,23...|[[-1.775142435E9]...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf = cdf.drop('hashes')\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10, seed=123)\n",
    "model = mh.fit(cdf)\n",
    "cdf = model.transform(cdf)\n",
    "cdf.select(\"NPI\", \"features\", \"hashes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npi=1578547329\n",
    "key = cdf.filter('NPI = {}'.format(npi)).select('NPI', 'features').collect()\n",
    "# print(type(key[0][1])) # <class 'pyspark.ml.linalg.SparseVector'>\n",
    "# print(key[0][1],'\\n') # (6,[2,3,5],[1.0,1.0,1.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar to 1578547329: [1184602823, 1962408021, 1962408898, 1083675995, 1437103702, 1740243138, 1144273475, 1437113578, 1821049750, 1922082676]\n"
     ]
    }
   ],
   "source": [
    "neighbors=10\n",
    "nn = model.approxNearestNeighbors(cdf, key[0][1], neighbors+1, distCol='JaccardDistance').select('NPI').collect()\n",
    "print('Top {} most similar to {}:'.format(neighbors, npi), [n[0] for n in nn[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 hash tables\n",
    "\n",
    "# Top 10 most similar to 1679576722: [1679576722, 1770586224, 1881686475, 1548219223, 1780634568, \\\n",
    "#                                     1194775270, 1104876366, 1336198001, 1336199561, 1376597120]\n",
    "\n",
    "# Top 10 most similar to 1578547329: [1184602823, 1962408021, 1962408898, 1083675995, 1437103702, \\\n",
    "#                                     1740243138, 1144273475, 1437113578, 1821049750, 1922082676]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/  \n",
    "https://databricks.com/blog/2017/05/09/detecting-abuse-scale-locality-sensitive-hashing-uber-engineering.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joining dataframes...\n",
    "# df.join(code_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib Similarity Matrix\n",
    "https://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191450300180579"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_rdd = sc.parallelize([[1, 2], [1, 5]])\n",
    "# RowMatrix(rdd, numRows=0, numCols=0)\n",
    "mat = RowMatrix(row_rdd)\n",
    "sims = mat.columnSimilarities()\n",
    "sims.entries.first().value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(features.take(1)[0]) # 938\n",
    "features.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = RowMatrix(features, numRows=5205376, numCols=938)\n",
    "sims = mat.columnSimilarities()\n",
    "sims.entries.first().value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF with jaccard metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(x):\n",
    "    return x-.2\n",
    "\n",
    "# dist_udf = udf(dist, ArrayType(IntegerType()))\n",
    "dist_udf = udf(dist, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| new|\n",
      "+----+\n",
      "| 0.8|\n",
      "| 0.8|\n",
      "|-0.2|\n",
      "| 0.8|\n",
      "| 0.8|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf = cdf.drop('new')\n",
    "cdf = cdf.withColumn('new', dist_udf('Entity_1'))\n",
    "cdf.select('new').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
